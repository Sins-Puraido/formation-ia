{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Création du jeu ###\n",
    "\n",
    "dans un premier temps on créer une class TicTacToe qui represente l'environement du jeu"
   ],
   "id": "e57f31a48cd4a758"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-08T08:46:46.594122Z",
     "start_time": "2024-11-08T08:46:45.599762Z"
    }
   },
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from jupyter_server.terminal import initialize\n",
    "\n",
    "\n",
    "class TicTacToeEnv:\n",
    "    def __init__(self):\n",
    "        self.board = [0] * 9  # 9 positions sur le plateau\n",
    "        self.current_winner = None\n",
    "\n",
    "    #reset les case du plateau\n",
    "    def reset(self):\n",
    "        self.board = [0] * 9\n",
    "        self.current_winner = None\n",
    "        return tuple(self.board)\n",
    "\n",
    "    #indique les case encore idsponible pour jouer son tour\n",
    "    def available_moves(self):\n",
    "        return [i for i, x in enumerate(self.board) if x == 0]\n",
    "\n",
    "    #pose un pion sur le plateau, renvois false si la case est deja prise et renvois true si le coup fait ganger le joueur\n",
    "    def make_move(self, position, player):\n",
    "        if self.board[position] == 0:\n",
    "            self.board[position] = player\n",
    "            if self.check_winner(position, player):\n",
    "                self.current_winner = player\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    #liste des combinaison gagnants\n",
    "    def check_winner(self, square, player):\n",
    "        # Conditions de victoire\n",
    "        win_conditions = [\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # lignes\n",
    "            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # colonnes\n",
    "            [0, 4, 8], [2, 4, 6]  # diagonales\n",
    "        ]\n",
    "        for condition in win_conditions:\n",
    "            if square in condition and all(self.board[i] == player for i in condition):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    #indique si la partie est une egalité\n",
    "    def is_draw(self):\n",
    "        return 0 not in self.board and self.current_winner is None\n",
    "    \n",
    "    #renvois l'etiat actuel du tableau, used dasn le stockage de la Q-table\n",
    "    def get_state(self):\n",
    "        return tuple(self.board)\n",
    "\n",
    "\n",
    "    #affichage du plateau\n",
    "    def render(self):\n",
    "        # Affichage du plateau\n",
    "        for row in [self.board[i:i + 3] for i in range(0, 9, 3)]:\n",
    "            print(\" | \".join([str(x) if x != 0 else \" \" for x in row]))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guill\\AppData\\Local\\Temp\\ipykernel_5804\\829602291.py:3: DeprecationWarning: Terminals support has moved to `jupyter_server_terminals`\n",
      "  from jupyter_server.terminal import initialize\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T08:46:46.620856Z",
     "start_time": "2024-11-08T08:46:46.607326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def trainAiVsPreProgrammed(agent, pre_programmed_agent, env, episodes=10000, print_every=1000):\n",
    "    win_qlearning, win_preprogrammed, draw = 0, 0, 0  # Stats counters\n",
    "\n",
    "    for episode in range(1, episodes + 1):\n",
    "        state = env.reset()  # Reset the environment at the start of each game\n",
    "        done = False\n",
    "        turn = 1 if episode % 2 == 1 else -1  # Alternate starting player\n",
    "\n",
    "        while not done:\n",
    "            if turn == 1:  # Q-learning agent's turn\n",
    "                available_moves = env.available_moves()\n",
    "                action = agent.choose_action(state, available_moves)\n",
    "                env.make_move(action, agent.player)\n",
    "                next_state = env.get_state()\n",
    "\n",
    "                # Update Q-table based on outcome\n",
    "                if env.current_winner == agent.player:\n",
    "                    agent.update_q_table(state, action, reward=1, next_state=next_state, done=True)\n",
    "                    win_qlearning += 1\n",
    "                    done = True\n",
    "                elif env.is_draw():\n",
    "                    agent.update_q_table(state, action, reward=0.5, next_state=next_state, done=True)\n",
    "                    draw += 1\n",
    "                    done = True\n",
    "                else:\n",
    "                    agent.update_q_table(state, action, reward=0, next_state=next_state, done=False)\n",
    "                state = next_state\n",
    "\n",
    "            else:  # PreProgrammed agent's turn\n",
    "                available_moves = env.available_moves()\n",
    "                action = pre_programmed_agent.choose_action(state, env)\n",
    "                env.make_move(action, pre_programmed_agent.player)\n",
    "                state = env.get_state()\n",
    "\n",
    "                # Check if pre-programmed agent won or if it's a draw\n",
    "                if env.current_winner == pre_programmed_agent.player:\n",
    "                    agent.update_q_table(state, action, reward=-1, next_state=state, done=True)\n",
    "                    win_preprogrammed += 1\n",
    "                    done = True\n",
    "                elif env.is_draw():\n",
    "                    agent.update_q_table(state, action, reward=0.5, next_state=state, done=True)\n",
    "                    draw += 1\n",
    "                    done = True\n",
    "\n",
    "            # Alternate turn\n",
    "            turn *= -1\n",
    "\n",
    "        # Decay epsilon for exploration\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        # Print statistics every `print_every` episodes\n",
    "        if episode % print_every == 0:\n",
    "            print(f\"Episode {episode} - Wins Q-learning: {win_qlearning}, Wins PreProgrammed: {win_preprogrammed}, Draws: {draw}\")\n",
    "            win_qlearning, win_preprogrammed, draw = 0, 0, 0  # Reset counters\n",
    "\n",
    "    print(\"Training complete\")"
   ],
   "id": "b9cc5227649d61",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T08:50:01.327699Z",
     "start_time": "2024-11-08T08:50:01.312557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trainAiVsRandom(ai_agent, env, episodes, ai_starts):\n",
    "    \n",
    "    #on définie un joueur qui va jouer aléatoirement a chaque tour\n",
    "    def random_player_move(available_moves):\n",
    "        return random.choice(available_moves)\n",
    "\n",
    "    win, loss, draw = 0, 0, 0  # Compteurs pour les statistiques de jeu\n",
    "\n",
    "    for episode in range(1, episodes + 1):\n",
    "        state = env.reset()  # Réinitialise l'environnement pour chaque partie\n",
    "        done = False\n",
    "        turn = 1 if ai_starts else -1  # Si ai_starts est True, l'IA commence ; sinon, le joueur aléatoire commence\n",
    "\n",
    "        while not done:\n",
    "            if turn == 1:  # Tour de l'agent IA\n",
    "                available_moves = env.available_moves()\n",
    "                action = ai_agent.choose_action(state, available_moves)\n",
    "                env.make_move(action, ai_agent.player)\n",
    "                next_state = env.get_state()\n",
    "\n",
    "                # Vérifie le résultat après le coup de l'IA\n",
    "                if env.current_winner == ai_agent.player:\n",
    "                    ai_agent.update_q_table(state, action, reward=1, next_state=next_state, done=True)\n",
    "                    win += 1\n",
    "                    done = True\n",
    "                elif env.is_draw():\n",
    "                    ai_agent.update_q_table(state, action, reward=0.5, next_state=next_state, done=True)\n",
    "                    draw += 1\n",
    "                    done = True\n",
    "                else:\n",
    "                    ai_agent.update_q_table(state, action, reward=0, next_state=next_state, done=False)\n",
    "\n",
    "                state = next_state\n",
    "            else:  # Tour du joueur aléatoire\n",
    "                available_moves = env.available_moves()\n",
    "                action = random_player_move(available_moves)\n",
    "                env.make_move(action, -ai_agent.player)\n",
    "                next_state = env.get_state()\n",
    "\n",
    "                # Vérifie le résultat après le coup du joueur aléatoire\n",
    "                if env.current_winner == -ai_agent.player:\n",
    "                    ai_agent.update_q_table(state, action, reward=-1, next_state=next_state, done=True)\n",
    "                    loss += 1\n",
    "                    done = True\n",
    "                elif env.is_draw():\n",
    "                    ai_agent.update_q_table(state, action, reward=0.5, next_state=next_state, done=True)\n",
    "                    draw += 1\n",
    "                    done = True\n",
    "                else:\n",
    "                    # Pas de mise à jour pour le joueur aléatoire\n",
    "                    state = next_state\n",
    "\n",
    "            turn *= -1  # Change le tour\n",
    "\n",
    "        # Décroît l'exploration de l'IA après chaque partie\n",
    "        ai_agent.decay_epsilon()\n",
    "\n",
    "        # Affiche les statistiques tous les 1000 épisodes pour voir l'avancé de l'IA\n",
    "        if episode % 10000 == 0:\n",
    "            print(f\"Épisode {episode} - Victoires de l'IA: {win}, Défaites de l'IA: {loss}, Égalités: {draw}\")\n",
    "            win, loss, draw = 0, 0, 0  # Réinitialise les compteurs pour la prochaine série\n",
    "\n",
    "    print(\"Entraînement terminé\")"
   ],
   "id": "9ec0c8b55f7a10d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T08:52:48.591364Z",
     "start_time": "2024-11-08T08:52:48.574932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trainAivsAi(agent1, agent2, env, episodes=10000, print_every=1000):\n",
    "  \n",
    "    win_agent1, win_agent2, draw = 0, 0, 0  # Compteurs pour les statistiques\n",
    "\n",
    "    for episode in range(1, episodes + 1):\n",
    "        state = env.reset()  # Réinitialise l'environnement pour chaque partie\n",
    "        done = False\n",
    "        turn = 1  # 1 pour agent1, -1 pour agent2\n",
    "\n",
    "        while not done:\n",
    "            if turn == 1:  # Tour de l'agent1\n",
    "                available_moves = env.available_moves()\n",
    "                action = agent1.choose_action(state, available_moves)\n",
    "                env.make_move(action, agent1.player)\n",
    "                next_state = env.get_state()\n",
    "\n",
    "                # Vérifie le résultat après le coup de l'agent1\n",
    "                if env.current_winner == agent1.player:\n",
    "                    agent1.update_q_table(state, action, reward=1, next_state=next_state, done=True)\n",
    "                    agent2.update_q_table(state, action, reward=-1, next_state=next_state, done=True)\n",
    "                    win_agent1 += 1\n",
    "                    done = True\n",
    "                elif env.is_draw():\n",
    "                    agent1.update_q_table(state, action, reward=1, next_state=next_state, done=True)\n",
    "                    agent2.update_q_table(state, action, reward=1.5, next_state=next_state, done=True)\n",
    "                    draw += 1\n",
    "                    done = True\n",
    "                else:\n",
    "                    agent1.update_q_table(state, action, reward=0, next_state=next_state, done=False)\n",
    "                state = next_state\n",
    "            else:  # Tour de l'agent2\n",
    "                available_moves = env.available_moves()\n",
    "                action = agent2.choose_action(state, available_moves)\n",
    "                env.make_move(action, agent2.player)\n",
    "                next_state = env.get_state()\n",
    "\n",
    "                # Vérifie le résultat après le coup de l'agent2\n",
    "                if env.current_winner == agent2.player:\n",
    "                    agent1.update_q_table(state, action, reward=-2, next_state=next_state, done=True)\n",
    "                    agent2.update_q_table(state, action, reward=2, next_state=next_state, done=True)\n",
    "                    win_agent2 += 1\n",
    "                    done = True\n",
    "                elif env.is_draw():\n",
    "                    agent1.update_q_table(state, action, reward=1, next_state=next_state, done=True)\n",
    "                    agent2.update_q_table(state, action, reward=1.5, next_state=next_state, done=True)\n",
    "                    draw += 1\n",
    "                    done = True\n",
    "                else:\n",
    "                    agent2.update_q_table(state, action, reward=0, next_state=next_state, done=False)\n",
    "                state = next_state\n",
    "\n",
    "            turn *= -1  # Change le tour\n",
    "\n",
    "        # Décroît l'exploration de chaque IA après chaque partie\n",
    "        agent1.decay_epsilon()\n",
    "        agent2.decay_epsilon()\n",
    "\n",
    "        # Affiche les statistiques tous les `print_every` épisodes\n",
    "        if episode % print_every == 0:\n",
    "            print(f\"Épisode {episode} - Victoires Agent1: {win_agent1}, Victoires Agent2: {win_agent2}, Égalités: {draw}\")\n",
    "            win_agent1, win_agent2, draw = 0, 0, 0  # Réinitialise les compteurs pour la prochaine série\n",
    "\n",
    "    # Affiche les statistiques finales après l'entraînement complet\n",
    "    print(\"Entraînement terminé\")\n"
   ],
   "id": "4da327c60927909",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T08:46:46.670789Z",
     "start_time": "2024-11-08T08:46:46.662150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PreProgrammedAgent:\n",
    "    def __init__(self, player):\n",
    "        self.player = player  # 1 for X, -1 for O\n",
    "        self.opponent = -player  # Opposite player\n",
    "\n",
    "    def choose_action(self, board, env):\n",
    "        available_moves = env.available_moves()\n",
    "        \n",
    "        # Check if there's a winning move for the agent\n",
    "        for move in available_moves:\n",
    "            new_board = list(board)\n",
    "            new_board[move] = self.player\n",
    "            if env.check_winner(move, self.player):\n",
    "                return move  # Play the winning move\n",
    "\n",
    "        # Check if the opponent has a winning move to block\n",
    "        for move in available_moves:\n",
    "            new_board = list(board)\n",
    "            new_board[move] = self.opponent\n",
    "            if env.check_winner(move, self.opponent):\n",
    "                return move  # Block the opponent\n",
    "\n",
    "        # Look for a move that could lead to a win in the future\n",
    "        for move in available_moves:\n",
    "            new_board = list(board)\n",
    "            new_board[move] = self.player\n",
    "            if any(env.check_winner(future_move, self.player) for future_move in env.available_moves()):\n",
    "                return move\n",
    "\n",
    "        # If no strategic moves, play a random move\n",
    "        return random.choice(available_moves)"
   ],
   "id": "46961975c2851539",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T08:52:21.068517Z",
     "start_time": "2024-11-08T08:52:21.056139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, player, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.99):\n",
    "        self.q_table = {}  # Dictionnaire pour stocker Q-values\n",
    "        self.alpha = alpha  # Taux d'apprentissage\n",
    "        self.gamma = gamma  # Facteur de réduction\n",
    "        self.epsilon = epsilon  # Facteur d'exploration\n",
    "        self.epsilon_decay = epsilon_decay  # Décroissance de l'exploration\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        # Récupère la Q-value pour une action donnée dans un état donné\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "\n",
    "    \n",
    "    def choose_action(self, state, available_actions):\n",
    "        # Choisir une action selon la politique ε-greedy\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(available_actions)  # Exploration\n",
    "        else:\n",
    "            # Exploitation: choisir l'action avec la plus grande Q-value\n",
    "            q_values = [self.get_q_value(state, action) for action in available_actions]\n",
    "            max_q_value = max(q_values)\n",
    "            max_q_actions = [action for action in available_actions if self.get_q_value(state, action) == max_q_value]\n",
    "            return random.choice(max_q_actions)\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state, done):\n",
    "        # Mise à jour de la Q-value\n",
    "        max_future_q = max([self.get_q_value(next_state, a) for a in range(9)], default=0)\n",
    "        current_q = self.get_q_value(state, action)\n",
    "        if done:\n",
    "            new_q = reward\n",
    "        else:\n",
    "            new_q = current_q + self.alpha * (reward + self.gamma * max_future_q - current_q)\n",
    "        self.q_table[(state, action)] = new_q\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon *= self.epsilon_decay\n"
   ],
   "id": "2248203751093ecd",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T08:46:46.708033Z",
     "start_time": "2024-11-08T08:46:46.703507Z"
    }
   },
   "cell_type": "code",
   "source": "env = TicTacToeEnv()",
   "id": "e52c12850b244fe2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T08:46:46.732544Z",
     "start_time": "2024-11-08T08:46:46.727094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent_First = QLearningAgent()\n",
    "Smart_Agent = PreProgrammedAgent()\n"
   ],
   "id": "555a72c382f8e92e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T08:50:50.141700Z",
     "start_time": "2024-11-08T08:50:45.534422Z"
    }
   },
   "cell_type": "code",
   "source": "trainAiVsPreProgrammed(agent_First,Smart_Agent, env, 10000)\n",
   "id": "ac91a75e8b6f25f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 - Wins Q-learning: 605, Wins PreProgrammed: 318, Draws: 77\n",
      "Episode 2000 - Wins Q-learning: 640, Wins PreProgrammed: 305, Draws: 55\n",
      "Episode 3000 - Wins Q-learning: 637, Wins PreProgrammed: 293, Draws: 70\n",
      "Episode 4000 - Wins Q-learning: 636, Wins PreProgrammed: 291, Draws: 73\n",
      "Episode 5000 - Wins Q-learning: 616, Wins PreProgrammed: 325, Draws: 59\n",
      "Episode 6000 - Wins Q-learning: 620, Wins PreProgrammed: 300, Draws: 80\n",
      "Episode 7000 - Wins Q-learning: 635, Wins PreProgrammed: 297, Draws: 68\n",
      "Episode 8000 - Wins Q-learning: 626, Wins PreProgrammed: 316, Draws: 58\n",
      "Episode 9000 - Wins Q-learning: 617, Wins PreProgrammed: 309, Draws: 74\n",
      "Episode 10000 - Wins Q-learning: 637, Wins PreProgrammed: 295, Draws: 68\n",
      "Training complete\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T08:50:45.521413Z",
     "start_time": "2024-11-08T08:50:30.582017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainAiVsRandom(agent_First, env, 100000, False)\n",
    "trainAiVsRandom(agent_First, env, 100000, True)\n"
   ],
   "id": "cb0e3130b3bb1712",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épisode 10000 - Victoires de l'IA: 5772, Défaites de l'IA: 3572, Égalités: 656\n",
      "Épisode 20000 - Victoires de l'IA: 5735, Défaites de l'IA: 3688, Égalités: 577\n",
      "Épisode 30000 - Victoires de l'IA: 5788, Défaites de l'IA: 3615, Égalités: 597\n",
      "Épisode 40000 - Victoires de l'IA: 5666, Défaites de l'IA: 3729, Égalités: 605\n",
      "Épisode 50000 - Victoires de l'IA: 5780, Défaites de l'IA: 3597, Égalités: 623\n",
      "Épisode 60000 - Victoires de l'IA: 5745, Défaites de l'IA: 3670, Égalités: 585\n",
      "Épisode 70000 - Victoires de l'IA: 5728, Défaites de l'IA: 3630, Égalités: 642\n",
      "Épisode 80000 - Victoires de l'IA: 5742, Défaites de l'IA: 3636, Égalités: 622\n",
      "Épisode 90000 - Victoires de l'IA: 5746, Défaites de l'IA: 3602, Égalités: 652\n",
      "Épisode 100000 - Victoires de l'IA: 5766, Défaites de l'IA: 3603, Égalités: 631\n",
      "Entraînement terminé\n",
      "Épisode 10000 - Victoires de l'IA: 6821, Défaites de l'IA: 2496, Égalités: 683\n",
      "Épisode 20000 - Victoires de l'IA: 6776, Défaites de l'IA: 2555, Égalités: 669\n",
      "Épisode 30000 - Victoires de l'IA: 6899, Défaites de l'IA: 2432, Égalités: 669\n",
      "Épisode 40000 - Victoires de l'IA: 6881, Défaites de l'IA: 2475, Égalités: 644\n",
      "Épisode 50000 - Victoires de l'IA: 6871, Défaites de l'IA: 2467, Égalités: 662\n",
      "Épisode 60000 - Victoires de l'IA: 6818, Défaites de l'IA: 2514, Égalités: 668\n",
      "Épisode 70000 - Victoires de l'IA: 6822, Défaites de l'IA: 2479, Égalités: 699\n",
      "Épisode 80000 - Victoires de l'IA: 6745, Défaites de l'IA: 2606, Égalités: 649\n",
      "Épisode 90000 - Victoires de l'IA: 6857, Défaites de l'IA: 2516, Égalités: 627\n",
      "Épisode 100000 - Victoires de l'IA: 6828, Défaites de l'IA: 2493, Égalités: 679\n",
      "Entraînement terminé\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "16e03e675bb830e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T08:50:55.872386Z",
     "start_time": "2024-11-08T08:50:55.056745Z"
    }
   },
   "cell_type": "code",
   "source": "trainAivsAi(agent_First, agent_First, env)\n",
   "id": "348101451ff5649d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épisode 1000 - Victoires Agent1: 0, Victoires Agent2: 1000, Égalités: 0\n",
      "Épisode 2000 - Victoires Agent1: 0, Victoires Agent2: 1000, Égalités: 0\n",
      "Épisode 3000 - Victoires Agent1: 0, Victoires Agent2: 1000, Égalités: 0\n",
      "Épisode 4000 - Victoires Agent1: 0, Victoires Agent2: 1000, Égalités: 0\n",
      "Épisode 5000 - Victoires Agent1: 0, Victoires Agent2: 1000, Égalités: 0\n",
      "Épisode 6000 - Victoires Agent1: 0, Victoires Agent2: 1000, Égalités: 0\n",
      "Épisode 7000 - Victoires Agent1: 0, Victoires Agent2: 1000, Égalités: 0\n",
      "Épisode 8000 - Victoires Agent1: 0, Victoires Agent2: 1000, Égalités: 0\n",
      "Épisode 9000 - Victoires Agent1: 0, Victoires Agent2: 1000, Égalités: 0\n",
      "Épisode 10000 - Victoires Agent1: 0, Victoires Agent2: 1000, Égalités: 0\n",
      "Entraînement terminé\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
